# Dropbox/Drive |@CoinBase

* Whimsical: [link](https://whimsical.com/dropbox-Pxo3WmfgEvHq4MXhYeme84)

## 1. Requirement Gathering&#x20;

### 1.1 FRs

* upload/download
* user account
  * <mark style="color:yellow;">**->Discuss: **</mark> free/paid account
    * free users might have limited storage & limited bandwidth
* root folder : replicated to all user's devices
  * automatic synchronization between devices
* Access:
  * **owner** has full access: Read-Write(RW)
  * a **non-owner user** can only share file/folders what he owns
* file/folder sharing
  * owner specifies access to others while sharing
  * <mark style="color:yellow;">**->Discuss: **</mark> conflict resolution when 2 users update the same shared file (similar to google doc)
* max storage size: 15GB (as per GDrive)
  * once this limit is reached: user cant **write **anything
* system should support **offline **creation/update/deletion of files/folders
  * when this device comes back online; all these changes get reflected to other devices
* history of updates(versioning) & recovery to a version:
  * <mark style="color:yellow;">**->Several ways to do versioning possible:**</mark>
    * version to version (like git commit)
    * daily snapshot (like whatsapp doc)
  * <mark style="color:yellow;">**->How do determine max\_storage\_size:**</mark>
    * count each version in the same storage
    * multiple versions -> same file -> same storage

### 1.2 Out of Scope

* Analytics (data\_size & bandwidth)
* Security:
  * security while sharing data( encryption/HTTPs/etc)
  * security of data stored in cloud (encryption)
* Search across file/folders

### 1.3 NFRs

* Highly available & fault tolerant
* Scalable: system should scale with inc. in number of users
* Minimum bandwidth while file transfer
  * <mark style="color:yellow;">**->Discuss: **</mark>upload/download in chunks
  * if upload/download of any of the chunk fail; only retry for them (instead of the whole file)
* Minimum latency in cross-device synchronisation
* Replication Consistency: eventual&#x20;
* Ensure <mark style="color:orange;">**ACID-ity**</mark> of files stored 🟢
  * <mark style="color:yellow;">**->Discuss: **</mark>
  * <mark style="color:yellow;">**what does ACID means in case of Dropbox?**</mark>
    * **Atomic**: updates should be pushed as All or None
    * **Consistency: **users should not see any inconsistent ( intermediatory state which dont make sense & might cause error if updated) states
    * **Isolation: **
      * updates on 2 separate files dont effect each other
      * updates on single file by 2 users -> is a tricky problem to solve
        * uspe added offline wala feature&#x20;
        * ....<mark style="color:yellow;">**->**</mark> (rabbit-hole of discussion here)
    * **Durability: **files should be replicated across data centers to avoid data loss
  * <mark style="color:yellow;">**->NOTE: **</mark> just having a RDBMS doesnt ensure ACIDITY...duh
    * ACIDIty means different things in different applications, understand the logic before applying.
  * <mark style="color:orange;">**On ATOMICITY:**</mark>
    * when a file got changed in one device => all devices should show this version
    * **The ISSUE:**
      * i.e. lets say there are 10 chunks of a file which got changed
      * you will upload these 10 chunks one-by-one&#x20;
      * and other devices will download these 10 chunks one-by-one
      * when there are only 5 chunks downloaded yet on other file; the use will see that file in <mark style="color:orange;">**transient state**</mark>**.**
      * we dont want this.
      * \=> what we want is either all 10 chunks are displayed or None <mark style="color:orange;">(all or none : Atomicity)</mark>
    * **HOW TO SOLE THIS ISSUE ( & achieve Atomicity):**
      * make one <mark style="color:orange;">**temporary(hidden from user) file on client side**</mark>.
      * apply all those 10 change on this tmp file
      * Use an <mark style="color:orange;">**atomic file operation**</mark> (like <mark style="color:orange;">**link/rename**</mark>) to **switch** the original file with this file



## 2. BOTEC

### 2.1 Scale of System

* Total users = 500M
* DAU = 100M
* each user connects from 3 different devices
* 1 user has 200 files/photos
  * \=> Total files = 100B
* 10 requests per user per day
  * \=> 100M requests/day
* High write & read

### 2.2 Storage size estimation

* avg file size = 1MB
  * \=> total space reqd = 1MB\*100B = **100 PB**

## 3. APIs

* `POST/PUT: upload_file(user_token, file_metadata, file_content) -> file_id`
* `PUT: update_file(user_token, file_id, change_metadata, changed_segments)`
  * `changed_segments: `is input stream of changed chunks
* `delete_file(`...`)`
* `update_metadata(...)`
* `list_files(user_token, root_folder, page_size, page_token)`
* `share_file_or_folder(user_token, file_id, user_id) `
  * check persmissions of user internally before sharing

## 4. Tables

## 5. DBs choices(NoSQL/SQL)

* **Metadata DB:**
  * has to be \*\*ACID \*\*(for conflict resolve)
  * \=> SQL

## 6. Basic HLD

![](../../.gitbook/assets/screenshot-2021-08-28-at-7.17.52-pm.png)



## 7. Component Design

* <mark style="color:yellow;">**How versioning works?🟢**</mark>
  * **OPTION#1**: keep a separate copy in cloud after every change. ❌
    * **Issue with this approach:** in a 2GB file, even for a single char change; we'll have to keep a full 2GB file in cloud.&#x20;
    * \=> Huge wastage of resources
  * **OPTION#2: ** break down the file into **1000 chunks** (2MB each)✅
    * On first upload: all **1000 chunks**  & **1 metadata file** are uploaded to cloud
    * On subsequent changes; lets say only **chunk#1 & chunk#10** 1 were change; so **only these 2 chunks** & **metadata file** gets uploaded in **version2.0**
    * **Another adv:** the upload script can work in **parallel** to upload each chunk individually.
* <mark style="color:orange;">**Client:**</mark>
  * is configured to keep track of files inside the folder
  * Client Application monitors the workspace folder on the user’s machine and syncs all files/folders in it with the remote Cloud Storage
  * operations for the client:
    1. Upload and download files.
    2. Detect file changes in the workspace folder.
    3. Handle conflict due to offline or concurrent updates.
  * Based on the above considerations, we can divide our client into following **four parts**:
  * <mark style="color:orange;">**Watcher:**</mark>
    * watches over the root folder
    * gets **notified** when a **new file is added** to folder
    * Watcher passes this info (alog with the meta-data of changes) to **chunker & indexer**
  * <mark style="color:orange;">**Chunker**</mark>
    * breaks down the new file into chunks
    * calculate **hash of each chunk**
    * uploads all the chunks to **Cloud(S3)**
    * **passes** this info to **indexer**:
      * **Url** it got after uploading to S3
      * **hash** of each chunk
  * <mark style="color:orange;">**Indexer**</mark>
    * **receives** this info from **chunker:**
      * **Url** it got after uploading to S3
      * **hash** of each chunk
    * updates this info in i<mark style="color:orange;">**nternal DB**</mark> against that file for which those **hash** belong to
    * indexer also passes this info to <mark style="color:orange;">**sync service**</mark> via **messaging queue** for:
      * **conflict resolution(** updates could happen from multiple **Clients**, no?**)**
      * store metadata if device is **offline**
* **Sync Service:**
  * sends back the updated info of this file to all the **clients**
  * **Indexer** : receives this info & updates corresponding files in the **folder**
  * **=> this is how files remain in sync across all devices ✅ **

## 8. Detailed HLD

![](../../.gitbook/assets/screenshot-2021-08-28-at-7.17.59-pm.png)

##

